import os
import logging
import numpy as np
import tensorflow as tf
import pandas as pd
import json
from keras.models import load_model
from keras._tf_keras.keras.preprocessing.text import tokenizer_from_json  # Korrigiert

# Logging-Konfiguration
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


def load_preprocessed_keypoints(file_path):
    """Lädt die vorverarbeiteten Keypoints aus einer CSV-Datei."""
    df = pd.read_csv(file_path)

    # Nur Spalten mit "_x" und "_y" auswählen
    keypoints_x = df.filter(like="_x").values
    keypoints_y = df.filter(like="_y").values

    return keypoints_x, keypoints_y


def process_keypoints(keypoints_x, keypoints_y):
    """Skaliert und kombiniert Keypoints (X und Y)."""
    min_x = np.min(keypoints_x)
    max_x = np.max(keypoints_x)
    min_y = np.min(keypoints_y)
    max_y = np.max(keypoints_y)

    keypoints_x = (np.array(keypoints_x) - min_x) / (max_x - min_x)
    keypoints_y = (np.array(keypoints_y) - min_y) / (max_y - min_y)

    keypoints = np.concatenate([keypoints_x, keypoints_y])
    return keypoints.reshape(-1, 1)  # 1, -1


def load_tokenizer(tokenizer_path):
    """Lädt den Tokenizer aus einer JSON-Datei."""
    with open(tokenizer_path, 'r', encoding='utf-8') as f:
        tokenizer_json = json.load(f)
    tokenizer = tokenizer_from_json(tokenizer_json)

    # Manually set the "<start>" token to ID 0 (or any valid range within 0-34)
    if "<start>" not in tokenizer.word_index:
        tokenizer.word_index["<start>"] = 0  # Ensure "<start>" is mapped to 0

    # Debug: Print all tokens in the tokenizer
    print("Tokens im Tokenizer:", tokenizer.word_index)

    # Verify if the start token is properly added and has ID 0
    start_token_id = tokenizer.word_index.get("<start>", None)
    if start_token_id is not None:
        print(f"Start-Token ID für '<start>': {start_token_id}")
    else:
        print("Fehler: Das '<start>' Token wurde nicht korrekt hinzugefügt.")

    return tokenizer


def predict_gloss(model, tokenizer, keypoints):
    # Bereite die Eingabedaten für den Encoder vor
    encoder_input_data = keypoints  # Dein vorbereiteter Encoder-Input

    # Hol das Start-Token aus dem Tokenizer
    start_token = tokenizer.texts_to_sequences([["<start>"]])  # Wrap "<start>" in a list

    # Debug: Print the start token and check for empty list
    print(f"Start-Token für '<start>': {start_token}")

    # Ensure the start token is correctly loaded
    if len(start_token) == 0 or len(start_token[0]) == 0:
        raise ValueError("Das Start-Token wurde nicht korrekt geladen.")

    # Repliziere Start-Token für jedes Keypoint
    start_token_repeated = np.array([[start_token[0][0]]] * encoder_input_data.shape[0])

    # Überprüfe die Formen der Eingabedaten
    print(f"encoder_input_data shape: {encoder_input_data.shape}")
    print(f"decoder_input_data shape: {start_token_repeated.shape}")

    # Vorhersage durchführen
    prediction = model.predict([encoder_input_data, start_token_repeated])
    predicted_sequence = np.argmax(prediction, axis=-1)

    # Vorhergesagtes Gloss zurückgeben
    predicted_gloss = tokenizer.sequences_to_texts(predicted_sequence)
    return predicted_gloss[0]


def main():
    model_path = r"C:\Users\stefa\PycharmProjects\SignAI\models\trained_model_v2.h5"
    tokenizer_path = r"C:\Users\stefa\PycharmProjects\SignAI\tokenizers\gloss_tokenizer.json"
    keypoints_file = r"C:\Users\stefa\PycharmProjects\SignAI\data\live_data\live_keypoints.csv"

    if not os.path.exists(model_path):
        logging.error(f"Modelldatei nicht gefunden: {model_path}")
        return
    if not os.path.exists(tokenizer_path):
        logging.error(f"Tokenizer-Datei nicht gefunden: {tokenizer_path}")
        return
    if not os.path.exists(keypoints_file):
        logging.error(f"Keypoints-Datei nicht gefunden: {keypoints_file}")
        return

    model = load_model(model_path)
    logging.info("Modell erfolgreich geladen.")

    tokenizer = load_tokenizer(tokenizer_path)
    logging.info("Tokenizer erfolgreich geladen.")

    keypoints_x, keypoints_y = load_preprocessed_keypoints(keypoints_file)
    logging.info("Vorverarbeitete Keypoints erfolgreich geladen.")

    processed_keypoints = process_keypoints(keypoints_x, keypoints_y)
    gloss_prediction = predict_gloss(model, tokenizer, processed_keypoints)
    logging.info(f"Vorhergesagtes Gloss: {gloss_prediction}")


if __name__ == "__main__":
    main()
